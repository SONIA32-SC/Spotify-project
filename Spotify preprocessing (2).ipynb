{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd45f42-92ea-489b-bedc-2b0b75795222",
   "metadata": {},
   "source": [
    "Spotify preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e73533-5618-470f-b76b-fd6207991063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33cf9b-5dd0-47cf-9533-d13d28daaa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "\n",
    "Merged_prep = pd.read_csv('Merged_eda.csv')\n",
    "Merged_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976fecb-9657-4134-8f76-ad54a4361364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Unnamed columns\n",
    "\n",
    "Merged_prep= Merged_prep.drop(columns= ['Unnamed: 0.1','Unnamed: 0'], axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b2d92-4367-446a-91ef-f5419d4fe4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f1af8-6a0d-4397-a7ac-97419f1ad4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_prep.isnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dabc07dd-9d15-4cf6-a78c-70e1f1bc7161",
   "metadata": {},
   "source": [
    "Handling missing values in these columns:\n",
    "\n",
    "1. track_name is difficult to substitute with any other value. The entire column will not be useful to the model it will be dropped.\n",
    "2. GDP_year with missing values will be replaced with 0 and these values will be replaced with year values.The rest of the values which are float values will be converted into 'int' data type.\n",
    "3. GDP missing values will be replaced with 0 and these values will then be replaced by GDP values from other rows with similar year values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb77b44-86eb-4ceb-b56d-69ab9befbbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing track_name value\n",
    "\n",
    "Merged_prep = Merged_prep.drop('track_name',axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b3481-33aa-4502-856f-a6df05ae2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing GDP_year\n",
    "\n",
    "Merged_prep['GDP_year'].fillna(0, inplace=True)\n",
    "\n",
    "Merged_prep['GDP_year'] = Merged_prep['GDP_year'].apply(lambda x: year.values if x == '0' else x)\n",
    "Merged_prep['GDP_year'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32236d86-21d3-40c2-8e08-70469e5d0fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverting float GDP_year values to int\n",
    "\n",
    "Merged_prep['GDP_year']= Merged_prep['GDP_year'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7427dfd-2f88-491c-8947-0e1d911f722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling GDP values\n",
    "\n",
    "Merged_prep['GDP'].fillna(0, inplace=True)\n",
    "Merged_prep['GDP'] = Merged_prep['GDP'].apply(lambda x: GDP.values if x == '0' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166af7d0-5aca-421c-b414-eb84cdf1adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if changes have been effected\n",
    "\n",
    "Merged_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478658c-08ee-4539-9e87-2e25466bdb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_prep.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4e3e4-6d7b-4f23-9e28-cc9ee5e6f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_prep.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00ea9f-77f4-47d7-be78-2a5464ff0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "\n",
    "X= Merged_prep.drop(columns=['popularity'])\n",
    "y= Merged_prep['popularity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539468b9-6e6a-4505-94bc-05ac46f8b01c",
   "metadata": {},
   "source": [
    "The second stage of preprocessing requires that data is split into training and testing sets. To do this, the train/test split from sklearn.model_selection will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e456141-d82b-4620-a4f5-07a104bae9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train/test split to prepare training data\n",
    "\n",
    "len(Merged_prep) * 0.75, len(Merged_prep)* 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43551741-2f67-4af2-b757-3a9372f9fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Merged_prep.drop(columns=['popularity'],axis=1), \n",
    "                                                    Merged_prep.popularity, test_size=0.25, \n",
    "                                                    random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b38144-a965-47c0-b0fb-0111a4021819",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ff7df-9ef4-4dba-9d22-6cdcc9eae46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef89e6-a970-41df-8e9c-df77aab3d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify X_train values are all numeric\n",
    "\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8213f9-9b77-4577-ba84-942bee4ccd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify X_test values are all numeric\n",
    "\n",
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f9749-197a-4288-ab3c-3a3ff71a44f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine numerical and categorical columns\n",
    "\n",
    "num= X.select_dtypes(include=['int', 'float']).columns\n",
    "cat= X.select_dtypes(include=['object','bool']).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f0517-c73f-4d43-afae-a020091ec0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming columns using OneHotEncoder for categorical columns and StandardScaler for numerical columns.\n",
    "# Using ColumnTransformer which allows different columns to be transformed separately and the features generated by each transformer to be concatenated to form a single feature space\n",
    "\n",
    "col_transform = ColumnTransformer(transformers=[('scaler', StandardScaler(), ['year', 'acousticness', 'liveness', 'instrumentalness','duration_min', 'GDP']),\n",
    "        ('encoder', OneHotEncoder(), ['genre'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f6d77-f2e4-4f09-b9a0-26d55e985ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using fit_transform on Merged_model dataframe to fit and trandform columns using ColumnTransform defined above \n",
    "\n",
    "col_transformed = col_transform.fit_transform(Merged_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f2b14-9c4c-4347-9aca-0a1274566582",
   "metadata": {},
   "source": [
    "At the next stage of the project, models will be choosen and a pipeline which contains all the steps required in preparing the data and the model will be defined. \n",
    "\n",
    "For the proposed predictive model, linear regression and logistic regression models will be tested. For the linear regression model, it is necessary to apply Ridge regression instead because of the detection of multicollinearity between some of the predictor variables at eda stage of the project.\n",
    "\n",
    "For both the Ridge Regression and logistic regression models that will be tested this dataset, cross_validate from sklearn.model_selection will be used to fit and assess the models performance.\n",
    "Both models will be assessed using: \n",
    "1. Mean Absolute Error scores which will be calculated for both.\n",
    "2. Coefficient of determination/r2\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93822c1-298c-4fff-bfd9-548d403bfc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving train and test sets\n",
    "\n",
    "with open('train.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train, y_train, col_transform), f)\n",
    "\n",
    "with open('test.pkl', 'wb') as f:\n",
    "    pickle.dump((X_test, y_test, col_transform), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c7a269-1900-4452-80f8-87b74fe9623f",
   "metadata": {},
   "source": [
    "# SUMMARY\n",
    "\n",
    "At the preprocessing stage of the project, missing values were handled by dropping an entire column(track_name) and by replacing missing values with 0 in GDP and GDP_year columns.\n",
    "\n",
    "X and y variables were defined and data was split into training and test sets in the ratio of 75:25.\n",
    "\n",
    "Numerical and categorical columns were defined. Numerical columns were transformed using the StandardScaler and categorical columns were transformed using the OneHotEncoder. These transfromations were combined into a single feature space with ColumnTransformer.\n",
    "\n",
    "ColumnTransformer was used to fit and transform dataframe.\n",
    "\n",
    "Training and Test sets together with Col_Transformer weresaved using the pickle module"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
